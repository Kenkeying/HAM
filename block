import math
import torch
import torch.nn as nn
import torch.nn.functional as F
from timm.models.layers import DropPath, trunc_normal_
from typing import Type
from timm.models.efficientnet_blocks import SqueezeExcite, DepthwiseSeparableConv


class SimAM(torch.nn.Module):
    def __init__(self, channels = None, e_lambda = 1e-4):
        super(SimAM, self).__init__()

        self.activaton = nn.Sigmoid()
        self.e_lambda = e_lambda

    def __repr__(self):
        s = self.__class__.__name__ + '('
        s += ('lambda=%f)' % self.e_lambda)
        return s

    @staticmethod
    def get_module_name():
        return "simam"

    def forward(self, x):

        b, c, h, w = x.size()
        
        n = w * h - 1

        x_minus_mu_square = (x - x.mean(dim=[2,3], keepdim=True)).pow(2)
        y = x_minus_mu_square / (4 * (x_minus_mu_square.sum(dim=[2,3], keepdim=True) / n + self.e_lambda)) + 0.5

        return x * self.activaton(y)
        
        
def _gelu_ignore_parameters(
        *args,
        **kwargs
) -> nn.Module:
    """ Bad trick to ignore the inplace=True argument in the DepthwiseSeparableConv of Timm.

    Args:
        *args: Ignored.
        **kwargs: Ignored.

    Returns:
        activation (nn.Module): GELU activation function.
    """
    activation = nn.GELU()
    return activation
    
    
class MBConvBlock(nn.Module):
    """
        Args:
        in_channels (int): Number of input channels.
        out_channels (int): Number of output channels.
        downscale (bool, optional): If true downscale by a factor of two is performed. Default: False
        act_layer (Type[nn.Module], optional): Type of activation layer to be utilized. Default: nn.GELU
        norm_layer (Type[nn.Module], optional): Type of normalization layer to be utilized. Default: nn.BatchNorm2d
        drop_path (float, optional): Dropout rate to be applied during training. Default 0.
    """

    def __init__(
            self,
            in_channels: int,
            out_channels: int,
            downscale: bool = False,
            act_layer: Type[nn.Module] = nn.GELU,
            norm_layer: Type[nn.Module] = nn.BatchNorm2d,
            drop_path: float = 0.,
            expand_ratio: int = 4.,
            use_se=False,
    ) -> None:
        """ Constructor method """
        # Call super constructor
        super(MBConvBlock, self).__init__()
        # Save parameter
        # Make main path
        self.main_path = nn.Sequential(
            nn.Conv2d(in_channels, in_channels, kernel_size=3, padding=1, stride=1, groups=in_channels),
            nn.BatchNorm2d(in_channels),
            nn.GELU(),

            nn.Conv2d(in_channels, in_channels, kernel_size=1, stride=1, groups=1),
            nn.BatchNorm2d(in_channels),
            nn.GELU(),
#            SimAM(in_channels),            
            )            
    def forward(self,x):
        x = self.main_path(x)
        output = x
        return output  
        

class Attention(nn.Module):
    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0., sr_ratio=1):
        super().__init__()
        assert dim % num_heads == 0, f"dim {dim} should be divided by num_heads {num_heads}."

        self.dim = dim
        self.num_heads = num_heads
        head_dim = dim // num_heads
        self.scale = qk_scale or head_dim ** -0.5

        self.q = nn.Linear(dim, dim, bias=qkv_bias)

        self.attn_drop = nn.Dropout(attn_drop)
        self.proj = nn.Linear(dim, dim)
        self.proj_drop = nn.Dropout(proj_drop)

        self.sr_ratio = sr_ratio
        if sr_ratio > 1:
            self.act = nn.GELU()
            if sr_ratio == 8:
                self.sr1 = nn.Conv2d(dim, dim, kernel_size=8, stride=8)
                self.norm1 = nn.LayerNorm(dim)
                self.sr2 = nn.Conv2d(dim, dim, kernel_size=4, stride=4)
                self.norm2 = nn.LayerNorm(dim)
                self.sr3 = nn.Conv2d(dim, dim, kernel_size=2, stride=2)
                self.norm3 = nn.LayerNorm(dim)
                self.sr4 = nn.Conv2d(dim, dim, kernel_size=1, stride=1)
                self.norm4 = nn.LayerNorm(dim)                
            if sr_ratio == 4:
                self.sr1 = nn.Conv2d(dim, dim, kernel_size=4, stride=4)
                self.norm1 = nn.LayerNorm(dim)
                self.sr2 = nn.Conv2d(dim, dim, kernel_size=2, stride=2)
                self.norm2 = nn.LayerNorm(dim)
            if sr_ratio == 2:
                self.sr1 = nn.Conv2d(dim, dim, kernel_size=2, stride=2)
                self.norm1 = nn.LayerNorm(dim)
                self.sr2 = nn.Conv2d(dim, dim, kernel_size=1, stride=1)
                self.norm2 = nn.LayerNorm(dim)
            self.kv1 = nn.Linear(dim, dim, bias=qkv_bias)
            self.kv2 = nn.Linear(dim, dim, bias=qkv_bias)
            self.kv3 = nn.Linear(dim, dim, bias=qkv_bias)
            self.kv4 = nn.Linear(dim, dim, bias=qkv_bias)
            self.local_conv1 = nn.Conv2d(dim//4, dim//4, kernel_size=3, padding=1, stride=1)
            self.local_conv2 = nn.Conv2d(dim//4, dim//4, kernel_size=3, padding=1, stride=1)
            self.local_conv3 = nn.Conv2d(dim//4, dim//4, kernel_size=3, padding=1, stride=1)
            self.local_conv4 = nn.Conv2d(dim//4, dim//4, kernel_size=3, padding=1, stride=1)
        else:
            self.kv = nn.Linear(dim, dim * 2, bias=qkv_bias)
            self.local_conv = nn.Conv2d(dim, dim, kernel_size=3, padding=1, stride=1, groups=dim)
        self.apply(self._init_weights)


    def forward(self, x, H, W):
#        B, N, C = x.shape
        
        B, N, C = x.shape
        
        q = self.q(x).reshape(B, N, self.num_heads, C // self.num_heads).permute(0, 2, 1, 3)
#        print(q.size())
        if self.sr_ratio > 1:
                x_ = x.permute(0, 2, 1).reshape(B, C, H, W)
                x_1 = self.act(self.norm1(self.sr1(x_).reshape(B, C, -1).permute(0, 2, 1)))
                x_2 = self.act(self.norm2(self.sr2(x_).reshape(B, C, -1).permute(0, 2, 1)))
                x_3 = self.act(self.norm3(self.sr3(x_).reshape(B, C, -1).permute(0, 2, 1)))
                x_4 = self.act(self.norm4(self.sr4(x_).reshape(B, C, -1).permute(0, 2, 1)))
                kv1 = self.kv1(x_1).reshape(B, -1, 4, self.num_heads//4, C // self.num_heads).permute(2, 0, 3, 1, 4)
                kv2 = self.kv2(x_2).reshape(B, -1, 4, self.num_heads//4, C // self.num_heads).permute(2, 0, 3, 1, 4)
                kv3 = self.kv3(x_3).reshape(B, -1, 4, self.num_heads//4, C // self.num_heads).permute(2, 0, 3, 1, 4)
                kv4 = self.kv4(x_4).reshape(B, -1, 4, self.num_heads//4, C // self.num_heads).permute(2, 0, 3, 1, 4)
                k1, v1 = kv1[0], kv1[1] #B head N C
                k2, v2 = kv2[0], kv2[1]
                k3, v3 = kv3[0], kv3[1]
                k4, v4 = kv4[0], kv4[1]
#                q1 = q[:, :self.num_heads//2]
#                q2 = q[:, self.num_heads//2:]
                attn1 = (q[:, :self.num_heads//4] @ k1.transpose(-2, -1)) * self.scale
                attn1 = attn1.softmax(dim=-1)
                attn1 = self.attn_drop(attn1)
                v1 = v1 + self.local_conv1(v1.transpose(1, 2).reshape(B, -1, C//4).
                                        transpose(1, 2).view(B, C//4, H//self.sr_ratio, W//self.sr_ratio)).\
                    view(B, C//4, -1).view(B, self.num_heads//4, C // self.num_heads, -1).transpose(-1, -2)
                x1 = (attn1 @ v1).transpose(1, 2).reshape(B, N, C//4)
                attn2 = (q[:, self.num_heads//4: (self.num_heads//4)*2] @ k2.transpose(-2, -1)) * self.scale
                attn2 = attn2.softmax(dim=-1)
                attn2 = self.attn_drop(attn2)
                v2 = v2 + self.local_conv2(v2.transpose(1, 2).reshape(B, -1, C//4).
                                        transpose(1, 2).view(B, C//4, H*2//self.sr_ratio, W*2//self.sr_ratio)).\
                    view(B, C//4, -1).view(B, self.num_heads//4, C // self.num_heads, -1).transpose(-1, -2)
                x2 = (attn2 @ v2).transpose(1, 2).reshape(B, N, C//4)
                attn3 = (q[:, (self.num_heads//4)*2:(self.num_heads//4)*3] @ k3.transpose(-2, -1)) * self.scale
                attn3 = attn3.softmax(dim=-1)
                attn3 = self.attn_drop(attn3)
                v3 = v3 + self.local_conv3(v3.transpose(1, 2).reshape(B, -1, C//4).
                                        transpose(1, 2).view(B, C//4, H*4//self.sr_ratio, W*4//self.sr_ratio)).\
                    view(B, C//4, -1).view(B, self.num_heads//4, C // self.num_heads, -1).transpose(-1, -2)
                x3 = (attn3 @ v3).transpose(1, 2).reshape(B, N, C//4)
                attn4 = (q[:, (self.num_heads//4)*3:] @ k4.transpose(-2, -1)) * self.scale
                attn4 = attn4.softmax(dim=-1)
                attn4 = self.attn_drop(attn4)
                v4 = v4 + self.local_conv4(v4.transpose(1, 2).reshape(B, -1, C//4).
                                        transpose(1, 2).view(B, C//4, H*8//self.sr_ratio, W*8//self.sr_ratio)).\
                    view(B, C//4, -1).view(B, self.num_heads//4, C // self.num_heads, -1).transpose(-1, -2)
                x4 = (attn4 @ v4).transpose(1, 2).reshape(B, N, C//4)                 

                x = torch.cat([x1, x2, x3, x4], dim=-1)
        else:
            kv = self.kv(x).reshape(B, -1, 2, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)
            k, v = kv[0], kv[1]

            attn = (q @ k.transpose(-2, -1)) * self.scale
            attn = attn.softmax(dim=-1)
            attn = self.attn_drop(attn)

            x = (attn @ v).transpose(1, 2).reshape(B, N, C) + self.local_conv(v.transpose(1, 2).reshape(B, N, C).
                                        transpose(1, 2).view(B, C, H, W)).view(B, C, N).transpose(1, 2)
        x = self.proj(x)
        x = self.proj_drop(x)

        return x       

class Attention2(nn.Module):
    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0., sr_ratio=1):
        super().__init__()
        assert dim % num_heads == 0, f"dim {dim} should be divided by num_heads {num_heads}."

        self.num_heads = num_heads
        head_dim = dim // num_heads
        self.scale = qk_scale or head_dim ** -0.5

        self.q = nn.Linear(dim, dim, bias=qkv_bias)

        self.attn_drop = nn.Dropout(attn_drop)
        self.proj = nn.Linear(dim//2, dim)
        self.proj_drop = nn.Dropout(proj_drop)

        self.sr_ratio = sr_ratio
        if sr_ratio > 1:
            self.act = nn.GELU()
            if sr_ratio == 8:
                self.sr1 = nn.Conv2d(dim, dim, kernel_size=8, stride=8)
                self.bn1 = nn.LayerNorm(dim)
                self.sr2 = nn.Conv2d(dim, dim, kernel_size=4, stride=4)
                self.bn2 = nn.LayerNorm(dim)
            if sr_ratio == 4:
                self.sr1 = nn.Conv2d(dim, dim, kernel_size=4, stride=4)
                self.bn1 = nn.LayerNorm(dim)
                self.sr2 = nn.Conv2d(dim, dim, kernel_size=2, stride=2)
                self.norm2 = nn.LayerNorm(dim)
            if sr_ratio == 2:
                self.sr1 = nn.Conv2d(dim, dim, kernel_size=2, stride=2)
#                self.sr1 = nn.AvgPool2d(2, 2)
                self.norm1 = nn.LayerNorm(dim)
                self.sr2 = nn.Conv2d(dim, dim, kernel_size=1, stride=1)
#                self.sr2 = nn.AvgPool2d(1, 1)
                self.norm2 = nn.LayerNorm(dim)
            self.kv1 = nn.Linear(dim, dim, bias=qkv_bias)
            self.kv2 = nn.Linear(dim, dim, bias=qkv_bias)
            self.local_conv1 = nn.Conv2d(dim//2, dim//2, kernel_size=3, padding=1, stride=1, groups=dim//2)
            self.local_conv2 = nn.Conv2d(dim//2, dim//2, kernel_size=3, padding=1, stride=1, groups=dim//2)
        else:
            self.kv = nn.Linear(dim, dim * 2, bias=qkv_bias)
            self.local_conv = nn.Conv2d(dim, dim, kernel_size=3, padding=1, stride=1, groups=dim)
 
        self.weight1 = nn.Parameter(torch.ones(2, dtype=torch.float32), requires_grad=True)
#        self.weight1_relu = nn.ReLU(inplace=True)
#        alpha1 = torch.FloatTensor([0.5])
#        self.alpha1 = nn.Parameter(alpha1)  # nn.Parameter is special Variable
#        alpha2 = torch.FloatTensor([0.5])
#        self.alpha2 = nn.Parameter(alpha2)  # nn.Parameter is special Variable         


    def forward(self, x, H, W):
#        B, N, C = x.shape
        
        B, N, C = x.shape
        
        q = self.q(x).reshape(B, N, self.num_heads, C // self.num_heads).permute(0, 2, 1, 3)
        if self.sr_ratio > 1:
                x_ = x.permute(0, 2, 1).reshape(B, C, H, W)
                x_1 = self.act(self.norm1(self.sr1(x_).reshape(B, C, -1).permute(0, 2, 1)))
                x_2 = self.act(self.norm2(self.sr2(x_).reshape(B, C, -1).permute(0, 2, 1)))
#                x_1 = self.sr1(x_).reshape(B, C, -1).permute(0, 2, 1)
#                x_2 = self.sr2(x_).reshape(B, C, -1).permute(0, 2, 1)               
                kv1 = self.kv1(x_1).reshape(B, -1, 2, self.num_heads//2, C // self.num_heads).permute(2, 0, 3, 1, 4)
                kv2 = self.kv2(x_2).reshape(B, -1, 2, self.num_heads//2, C // self.num_heads).permute(2, 0, 3, 1, 4)
                k1, v1 = kv1[0], kv1[1] #B head N C
                k2, v2 = kv2[0], kv2[1]
                attn1 = (q[:, :self.num_heads//2] @ k1.transpose(-2, -1)) * self.scale
                attn1 = attn1.softmax(dim=-1)
                attn1 = self.attn_drop(attn1)
                v1 = v1 + self.local_conv1(v1.transpose(1, 2).reshape(B, -1, C//2).
                                        transpose(1, 2).view(B, C//2, H//self.sr_ratio, W//self.sr_ratio)).\
                    view(B, C//2, -1).view(B, self.num_heads//2, C // self.num_heads, -1).transpose(-1, -2)
                x1 = (attn1 @ v1).transpose(1, 2).reshape(B, N, C//2)
                attn2 = (q[:, self.num_heads // 2:] @ k2.transpose(-2, -1)) * self.scale
                attn2 = attn2.softmax(dim=-1)
                attn2 = self.attn_drop(attn2)
                v2 = v2 + self.local_conv2(v2.transpose(1, 2).reshape(B, -1, C//2).
                                        transpose(1, 2).view(B, C//2, H*2//self.sr_ratio, W*2//self.sr_ratio)).\
                    view(B, C//2, -1).view(B, self.num_heads//2, C // self.num_heads, -1).transpose(-1, -2)
                x2 = (attn2 @ v2).transpose(1, 2).reshape(B, N, C//2)
                w1 = F.relu(self.weight1)
                weight2 = w1 / (torch.sum(w1, dim=0) + 1e-4)
                x1 = x1 * weight2[0]
                x2 = x2 * weight2[1]
                x = x1 + x2
#                x = torch.cat([x1, x2], dim=-1)
        else:
            kv = self.kv(x).reshape(B, -1, 2, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)
            k, v = kv[0], kv[1]

            attn = (q @ k.transpose(-2, -1)) * self.scale
            attn = attn.softmax(dim=-1)
            attn = self.attn_drop(attn)

            x = (attn @ v).transpose(1, 2).reshape(B, N, C) + self.local_conv(v.transpose(1, 2).reshape(B, N, C).
                                        transpose(1, 2).view(B, C, H, W)).view(B, C, N).transpose(1, 2)
        x = self.proj(x)
        x = self.proj_drop(x)

        return x

def channel_shuffle(x, groups):
    batchsize, num_channels, height, width = x.data.size()
    assert (num_channels % groups == 0)
    channels_per_group = num_channels // groups
    # reshape
    x = x.view(batchsize, groups, channels_per_group, height, width)

    # transpose
    # - contiguous() required if transpose() is used before view().
    #   See https://github.com/pytorch/pytorch/issues/764
    x = torch.transpose(x, 1, 2).contiguous()

    # flatten
    x = x.view(batchsize, -1, height, width)

    return x
    
        
class Block(nn.Module):

    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,
                 drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm, sr_ratio=8):
        super().__init__()
        self.norm1 = norm_layer(dim)
        self.attn = Attention2(dim, num_heads, qkv_bias, qk_scale, attn_drop, drop, sr_ratio)
        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()
        self.mlp = MBConvBlock(in_channels=dim//2, out_channels=dim//2, act_layer=act_layer, drop_path=drop)
        self.left_part = round(0.5 * dim)      


    def forward(self, x):
    
        
        B,C,H,W = x.shape
        
        # x1 = x[:, :self.left_part, :, :]
        # x2 = x[:, self.left_part:, :, :]        
        # x2 = self.mlp(x2)
#        output = self.conv_proj(torch.cat((x1,x2), dim=1)
#                )
#        x_1 = channel_shuffle(torch.cat((x2, x1), 1), 2)        

        # B,C,H,W
        x0 = x.flatten(2).transpose(1, 2).contiguous()
        x0 = x0 + self.drop_path(self.attn(self.norm1(x0), H, W))
        x_1 = x0.transpose(1, 2).contiguous().view(B, C, H, W)
        x1 = x_1[:, :self.left_part, :, :]
        x2 = x_1[:, self.left_part:, :, :]        
        x2 = self.mlp(x2)
        output = channel_shuffle(torch.cat((x2, x1), 1), 2)        
#        output = output
        return output
 
 
class pvt3_3_1(nn.Module):
    def __init__(self, cin=512, depth=3,
                 num_heads=4, mlp_ratios=4, qkv_bias=True, qk_scale=None, drop_rate=0.,
                 attn_drop_rate=0., drop_path_rate=0., norm_layer=nn.LayerNorm,
                 sr_ratios=2):
        super().__init__()
        self.depth = depth

        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]
        cur = 0

        self.block = nn.ModuleList([Block(
                dim=cin, num_heads=num_heads, mlp_ratio=mlp_ratios, qkv_bias=qkv_bias, qk_scale=qk_scale,
                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[cur + j], norm_layer=norm_layer,
                sr_ratio=sr_ratios)
                for j in range(depth)])
        cur += depth


    def forward(self, x):
        B = x.shape[0]
        for blk in self.block:
            x = blk(x)

        return x 
